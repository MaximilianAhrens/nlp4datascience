{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP4datascience Module - Tutorial 1: Text Preprocessing\n",
    "This python module's purpose is to facilitate the use of natural language inputs (ie. text) in data science and data analysis. For the foundational implementaiton, it makes use of popular NLP elements in the python modules NLTK, Sci-Kit Learn and Gensim and offers further proprietary functionalities beyond those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from tqdm import tqdm\n",
    "from nlp4datascience.datahandling.largepickle import pickle_load, pickle_dump\n",
    "from nlp4datascience.preprocessors.nlp4datascience import BagOfWords\n",
    "from nlp4datascience.preprocessors.nlp4datascience import DTM\n",
    "from gensim.models.phrases import Phrases, Phraser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<pathname>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text data preprocesing\n",
    "BagOfWords creates an nlp4datascience-object which lets us preprocess and tokenize our text data with a few simply commands.\n",
    "min_length defines the minimum character length of words being considered. Words with fewer characters are automatically being deleted.\n",
    "ngram_length defines the the degreee of ngrams we want to extract. Set this to 1 for now. We will extract bigrams, trigrams, ngrams later via a collocation approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = BagOfWords(text, min_length=1, ngram_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our NLP4datascience object, called \"preprocessed_text\".\n",
    "Let's clean the text by:\n",
    "1. removing stopwords (based on NLTK stopword list + our own list)\n",
    "2. lowercasing the corpus\n",
    "3. removing punctuation\n",
    "4. removing special characters (can be customized)\n",
    "5. removing numbers (can be switched off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to add our own stopwords to the stopword list, then add the words by using the following line of code or upload a list of words and then as preprocessed_text.custom_stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text.custom_stopwords = [\"my_stopword+1\",\"my_stopword_2\",...,\"my_stopword_N\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean and tokeinze our text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text.clean()\n",
    "preprocessed_text.tokenize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create unigrams via stemming (we could also use lemmatization instead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text.stemm() # use preprocessed_text.lemmatize() if you want to use this approach instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually inspect the preprocessed corpus. The graph plots the ranking of the ngrams either according to a tf, df or a tf-idf weighting. Let's choose the popular tf-idf ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text.visualize(weight = \"tf-idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we feel we want to cut the corpus at a certain minimum threshold of tf-idf scores, we can do this by using the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text.rank_remove(rank=\"tf-idf\", items=\"uni\", cutoff = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually inspect our corpus after applying the tf-idf cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessed_text.visualize(weight = \"tf-idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could even apply another cutoff criterion, say based on document frequency, on top of the former one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are happy with our results, we can save them to our computer. You can choose to save it as a pickle file (.pkl) or a csv file (.csv). The preprocessing is now completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text.save(\"pkl\", <path_where_to_save_the_file\", str(\"<filename>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Document-Term-Matrix (based on unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the preprocessed text corpus from step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = pickle_load(\"<preprocessed_data_from_step_1>.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DTM object based on the NLP4datascience module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_object = DTM(ngrams.uni_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the DTM based on unigrams and inspect tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_object.unigram_dtm()\n",
    "dtm_object.dtm\n",
    "dtm_object.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bigrams, we specify the minimum frequency of bigrams (and unigrams in them) occuring as well as a scoring-function threshold. The higher the threshold, the fewer bigrams are formed (this is based on the collocation approach, and on the gensim implementation of it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_object.bigram_dtm(min_count=10, threshold=10)\n",
    "dtm_object.dtm\n",
    "dtm_object.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ngrams (more specifically, up to 4-grams), we can use the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_object.ngram_dtm(min_count=10, threshold=10)\n",
    "dtm_object.dtm\n",
    "dtm_object.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save any of the DTMs and the corresponding vocabulary list to your computer, use the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz('<path_where_to_store_the_compressed_dtm>.npz', dtm_object.dtm)\n",
    "pd.Series(dtm_object.tokens).to_pickle('<path_where_to_save_the_file>.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to inspect the vocabulary, we can run the following command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note: \n",
    "The DTM is stored as a Compressed Sparse Row (CSR) scipy matrix. This is advantageous when the corpus is getting too large to be fully held in memory in an uncompressed form, for example, when we have millions of documents and (hundred) thousands of unique ngram terms.\n",
    "\n",
    "If you want to bring your compressed DTM into a non-compressed (ie. dense) format, use the below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_dense = dtm_sparse.todense()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
