{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP4datascience Module - Tutorial 1: Text Preprocessing\n",
    "This python module's purpose is to facilitate the use of natural language inputs (ie. text) in data science and data analysis. For the foundational implementaiton, it makes use of popular NLP elements in the python modules NLTK, Sci-Kit Learn and Gensim and offers further proprietary functionalities beyond those. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from tqdm import tqdm\n",
    "from nlp4datascience.datahandling.largepickle import pickle_load, pickle_dump\n",
    "from nlp4datascience.preprocessors.nlp4datascience import BagOfWords\n",
    "from nlp4datascience.preprocessors.nlp4datascience import DTM\n",
    "from gensim.models.phrases import Phrases, Phraser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<pathname>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text data preprocesing\n",
    "BagOfWords creates an nlp4datascience-object, which lets us preprocess and tokenize our text data with a few simple commands.\n",
    "The option \"min_length\" lets you define the minimum character length of words being considered. Words with fewer characters are automatically being deleted.\n",
    "For now we will be extracting unigrams only. We will extract bigrams, trigrams, ngrams later via a collocation approach (Mikholov et al, 2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text = BagOfWords(text, min_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have our NLP4datascience object, called \"preprocessed_text\".\n",
    "Let's clean the text by:\n",
    "1. removing stopwords (based on NLTK stopword list + our own list)\n",
    "2. lowercasing the corpus\n",
    "3. removing punctuation\n",
    "4. removing special characters (can be customized)\n",
    "5. removing numbers (can be switched off)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to add our own stopwords to the stopword list, then add the words by using the following line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text.custom_stopwords = [\"my_stopword+1\",\"my_stopword_2\",...,\"my_stopword_N\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's clean and tokeinze our text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text.clean()\n",
    "preprocessed_text.tokenize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create unigrams via stemming (we could also use lemmatization instead)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0703369a3062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocessed_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstemm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# use preprocessed_text.lemmatize() if you want to use lemmatization instead\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessed_text' is not defined"
     ]
    }
   ],
   "source": [
    "preprocessed_text.stemm() # use preprocessed_text.lemmatize() if you want to use lemmatization instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually inspect the preprocessed corpus. The graph plots the ranking of the ngrams either according to a tf, df or a tf-idf weighting. Let's choose the popular tf-idf ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text.visualize(weight = \"tf-idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to cut the corpus at a certain minimum threshold of tf-idf (or tf or df) scores, we can do this by using the following command. With \"weight\", we specify the ranking according to which we want to remove tokens. We choose \"tf-idf\" here. The cutoff sets the minimum ranking score. Tokens ranking lower than this cutoff, will be removed from the corpus. We can use the visual inspection from the previous step to obtain a good estimate for the value of the cutoff. The \"items\" option specifies which ngram-type we want to modify. As we are working with unigrams here, we specify it as \"uni\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text.remove_tokens(weight=\"tf-idf\", items=\"uni\", cutoff = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visually inspect our corpus after applying the tf-idf cutoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocessed_text.visualize_adj(weight = \"tf-idf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could even apply another cutoff criterion, say based on document frequency, on top of the former one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are happy with our results, we can save them to our computer:\n",
    "- \"output_directory\" defines the path where to save the file\n",
    "- \"filename\" defines the name of the saved file\n",
    "- \"data_format\" lets you choose to save the file either as a pickle file (.pkl) or as a csv file (.csv). If not specified, pickle is the default\n",
    "\n",
    "The preprocessing is now completed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_text.save(output_directory, \"filename\", data_format = \"pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Document-Term-Matrix (based on unigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the preprocessed text corpus from step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = pickle_load(\"<preprocessed_data_from_step_1>.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DTM object based on the NLP4datascience module. We can choose \".uni\" to use the unadjusted corpus, or \".uni_adj\" to use the corpus adjusted by our previously specified cutoff criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_uni_adj = DTM(ngrams.uni_adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the DTM based on unigrams and inspect the dtm and the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtm_uni_adj' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a219d99ec21a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtm_uni_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munigram_dtm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdtm_uni_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdtm_uni_adj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dtm_uni_adj' is not defined"
     ]
    }
   ],
   "source": [
    "dtm_uni_adj.unigram_dtm()\n",
    "dtm_uni_adj.dtm\n",
    "dtm_uni_adj.tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the dtm and the corresponding token vocabulary list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_uni_adj.save(output_directory, \"unigrams_adj\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bigrams, we specify the minimum frequency of bigrams (and unigrams in them) occuring as well as a scoring-function threshold. The higher the threshold, the fewer bigrams are formed (this is based on the collocation approach, and on the gensim implementation of it). \n",
    "\n",
    "**IMPORTANT**: Feed in the unadjusted unigram corpus for bigram, trigram, ngram DTM creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_bi = DTM(ngrams[\"uni\"])\n",
    "dtm_bi.ngram_dtm(min_count=10, threshold=10)\n",
    "dtm_bi.save(output_dir, \"bigrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ngrams (more specifically, up to 4-grams), we can use the following command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_ngrams = DTM(ngrams[\"uni\"])\n",
    "dtm_ngrams.ngram_dtm(min_count=10, threshold=10)\n",
    "dtm_ngrams.save(output_dir, \"ngrams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inspection and saving of the data works the same was as for the unigram-DTM described just above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Note: \n",
    "The DTM is stored as a Compressed Sparse Row (CSR) scipy matrix. This is advantageous when the corpus is getting too large to be fully held in memory in an uncompressed form, for example, when we have millions of documents and (hundred-) thousands of unique ngram terms.\n",
    "\n",
    "If you want to bring your compressed DTM into a non-compressed (ie. dense) format, use the below command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtm_dense = dtm_sparse.todense()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
